{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01043018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, Dropout, LSTM, GRU, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e133a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0a90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and sample 40% of the data\n",
    "data = pd.read_csv('flight_data_2021.csv')\n",
    "data_sampled = data.sample(frac=0.4, random_state=42) # 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da5b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampled.to_csv('sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c462cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTimeBlk</th>\n",
       "      <th>dept-type</th>\n",
       "      <th>dept-elevation_ft</th>\n",
       "      <th>Route_Popularity</th>\n",
       "      <th>Distance_Final</th>\n",
       "      <th>Wind_Speed_mph</th>\n",
       "      <th>Wind_Gust_mph</th>\n",
       "      <th>Visibility_miles</th>\n",
       "      <th>tempF</th>\n",
       "      <th>precip_in</th>\n",
       "      <th>daily_snow_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0700-0759</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>2941.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>422.246813</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0900-0959</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>422.246813</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1300-1359</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>266.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>1356.657710</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1600-1659</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>607.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1396.572313</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1800-1859</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>684.252343</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Airline  Cancelled  Quarter  Month  DayOfWeek DepTimeBlk  \\\n",
       "0  SkyWest Airlines Inc.      False        1      3          3  0700-0759   \n",
       "1  SkyWest Airlines Inc.      False        1      3          3  0900-0959   \n",
       "2  SkyWest Airlines Inc.      False        1      3          3  1300-1359   \n",
       "3  SkyWest Airlines Inc.      False        1      3          3  1600-1659   \n",
       "4  SkyWest Airlines Inc.      False        1      3          3  1800-1859   \n",
       "\n",
       "        dept-type  dept-elevation_ft  Route_Popularity  Distance_Final  \\\n",
       "0  medium_airport             2941.0             937.0      422.246813   \n",
       "1   large_airport             1135.0             937.0      422.246813   \n",
       "2   large_airport              266.0             918.0     1356.657710   \n",
       "3   large_airport              607.0             568.0     1396.572313   \n",
       "4   large_airport             1135.0            1135.0      684.252343   \n",
       "\n",
       "   Wind_Speed_mph  Wind_Gust_mph  Visibility_miles  tempF  precip_in  \\\n",
       "0            10.0           17.0               6.0     49        0.0   \n",
       "1             6.0            9.0               6.0     60        0.0   \n",
       "2            10.0           18.0               6.0     39        0.0   \n",
       "3            10.0           16.0               6.0     65        0.0   \n",
       "4            21.0           35.0               6.0     71        0.0   \n",
       "\n",
       "   daily_snow_in  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['Origin','Dest','DestState','OriginState','DepDelayMinutes',\n",
    "                   'Operated_or_Branded_Code_Share_Partners',\n",
    "                   'arr-type','arr-elevation_ft']\n",
    "clean_data = data.drop(columns_to_drop, axis=1)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f6d30",
   "metadata": {},
   "source": [
    "## Undersampling Major Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f5f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled\n",
      "False    106876\n",
      "True     106876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Applied Downsampling to balance the weight for minor class \"Cancelled\"\n",
    "data_majority = clean_data[clean_data['Cancelled'] == False]\n",
    "data_minority = clean_data[clean_data['Cancelled'] == True]\n",
    "\n",
    "# Step 3: Downsample the majority class\n",
    "data_majority_downsampled = resample(data_majority, replace=False, n_samples=len(data_minority), random_state=123)\n",
    "\n",
    "# Step 4: Combine minority class with downsampled majority class\n",
    "data_downsampled = pd.concat([data_majority_downsampled, data_minority])\n",
    "\n",
    "# Display new class counts\n",
    "print(data_downsampled['Cancelled'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff2f48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data_downsampled is your DataFrame with features and target\n",
    "# Separate features and target\n",
    "X = data_downsampled.drop(columns=['Cancelled'])\n",
    "y = data_downsampled['Cancelled']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8d405",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52dfd6c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 1, 50)             5300      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 1, 50)             0         \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,401\n",
      "Trainable params: 10,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "4676/4676 - 42s - loss: 0.6082 - accuracy: 0.6534 - val_loss: 0.5810 - val_accuracy: 0.6803 - 42s/epoch - 9ms/step\n",
      "Epoch 2/30\n",
      "4676/4676 - 39s - loss: 0.5845 - accuracy: 0.6803 - val_loss: 0.5688 - val_accuracy: 0.6913 - 39s/epoch - 8ms/step\n",
      "Epoch 3/30\n",
      "4676/4676 - 39s - loss: 0.5753 - accuracy: 0.6885 - val_loss: 0.5593 - val_accuracy: 0.7002 - 39s/epoch - 8ms/step\n",
      "Epoch 4/30\n",
      "4676/4676 - 38s - loss: 0.5675 - accuracy: 0.6962 - val_loss: 0.5535 - val_accuracy: 0.7041 - 38s/epoch - 8ms/step\n",
      "Epoch 5/30\n",
      "4676/4676 - 39s - loss: 0.5623 - accuracy: 0.7019 - val_loss: 0.5472 - val_accuracy: 0.7110 - 39s/epoch - 8ms/step\n",
      "Epoch 6/30\n",
      "4676/4676 - 40s - loss: 0.5578 - accuracy: 0.7054 - val_loss: 0.5472 - val_accuracy: 0.7122 - 40s/epoch - 9ms/step\n",
      "Epoch 7/30\n",
      "4676/4676 - 40s - loss: 0.5531 - accuracy: 0.7105 - val_loss: 0.5415 - val_accuracy: 0.7162 - 40s/epoch - 9ms/step\n",
      "Epoch 8/30\n",
      "4676/4676 - 40s - loss: 0.5507 - accuracy: 0.7107 - val_loss: 0.5395 - val_accuracy: 0.7216 - 40s/epoch - 9ms/step\n",
      "Epoch 9/30\n",
      "4676/4676 - 40s - loss: 0.5480 - accuracy: 0.7140 - val_loss: 0.5360 - val_accuracy: 0.7228 - 40s/epoch - 8ms/step\n",
      "Epoch 10/30\n",
      "4676/4676 - 39s - loss: 0.5451 - accuracy: 0.7160 - val_loss: 0.5343 - val_accuracy: 0.7232 - 39s/epoch - 8ms/step\n",
      "Epoch 11/30\n",
      "4676/4676 - 40s - loss: 0.5435 - accuracy: 0.7187 - val_loss: 0.5316 - val_accuracy: 0.7267 - 40s/epoch - 8ms/step\n",
      "Epoch 12/30\n",
      "4676/4676 - 39s - loss: 0.5405 - accuracy: 0.7214 - val_loss: 0.5300 - val_accuracy: 0.7287 - 39s/epoch - 8ms/step\n",
      "Epoch 13/30\n",
      "4676/4676 - 39s - loss: 0.5385 - accuracy: 0.7228 - val_loss: 0.5259 - val_accuracy: 0.7324 - 39s/epoch - 8ms/step\n",
      "Epoch 14/30\n",
      "4676/4676 - 38s - loss: 0.5352 - accuracy: 0.7255 - val_loss: 0.5236 - val_accuracy: 0.7340 - 38s/epoch - 8ms/step\n",
      "Epoch 15/30\n",
      "4676/4676 - 40s - loss: 0.5341 - accuracy: 0.7268 - val_loss: 0.5229 - val_accuracy: 0.7364 - 40s/epoch - 9ms/step\n",
      "Epoch 16/30\n",
      "4676/4676 - 40s - loss: 0.5331 - accuracy: 0.7284 - val_loss: 0.5205 - val_accuracy: 0.7377 - 40s/epoch - 8ms/step\n",
      "Epoch 17/30\n",
      "4676/4676 - 40s - loss: 0.5317 - accuracy: 0.7287 - val_loss: 0.5194 - val_accuracy: 0.7368 - 40s/epoch - 9ms/step\n",
      "Epoch 18/30\n",
      "4676/4676 - 40s - loss: 0.5307 - accuracy: 0.7302 - val_loss: 0.5189 - val_accuracy: 0.7372 - 40s/epoch - 9ms/step\n",
      "Epoch 19/30\n",
      "4676/4676 - 35s - loss: 0.5292 - accuracy: 0.7306 - val_loss: 0.5160 - val_accuracy: 0.7400 - 35s/epoch - 7ms/step\n",
      "Epoch 20/30\n",
      "4676/4676 - 28s - loss: 0.5290 - accuracy: 0.7305 - val_loss: 0.5150 - val_accuracy: 0.7434 - 28s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "4676/4676 - 28s - loss: 0.5274 - accuracy: 0.7320 - val_loss: 0.5145 - val_accuracy: 0.7407 - 28s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "4676/4676 - 28s - loss: 0.5269 - accuracy: 0.7318 - val_loss: 0.5144 - val_accuracy: 0.7435 - 28s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "4676/4676 - 29s - loss: 0.5260 - accuracy: 0.7342 - val_loss: 0.5129 - val_accuracy: 0.7442 - 29s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "4676/4676 - 28s - loss: 0.5248 - accuracy: 0.7348 - val_loss: 0.5133 - val_accuracy: 0.7425 - 28s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "4676/4676 - 28s - loss: 0.5248 - accuracy: 0.7342 - val_loss: 0.5128 - val_accuracy: 0.7428 - 28s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "4676/4676 - 28s - loss: 0.5254 - accuracy: 0.7344 - val_loss: 0.5140 - val_accuracy: 0.7414 - 28s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "4676/4676 - 28s - loss: 0.5233 - accuracy: 0.7353 - val_loss: 0.5119 - val_accuracy: 0.7424 - 28s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "4676/4676 - 28s - loss: 0.5234 - accuracy: 0.7362 - val_loss: 0.5104 - val_accuracy: 0.7449 - 28s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "4676/4676 - 29s - loss: 0.5233 - accuracy: 0.7344 - val_loss: 0.5124 - val_accuracy: 0.7448 - 29s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "4676/4676 - 28s - loss: 0.5229 - accuracy: 0.7356 - val_loss: 0.5124 - val_accuracy: 0.7453 - 28s/epoch - 6ms/step\n",
      "1002/1002 - 2s - loss: 0.5122 - accuracy: 0.7463 - 2s/epoch - 2ms/step\n",
      "Test Accuracy: 0.7463\n",
      "1002/1002 [==============================] - 2s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.71      0.74     15934\n",
      "        True       0.73      0.78      0.76     16129\n",
      "\n",
      "    accuracy                           0.75     32063\n",
      "   macro avg       0.75      0.75      0.75     32063\n",
      "weighted avg       0.75      0.75      0.75     32063\n",
      "\n",
      "[[11350  4584]\n",
      " [ 3551 12578]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape for LSTM [samples, timesteps, features]\n",
    "# Here, we assume each sample is treated as one timestep with all features\n",
    "# X_processed = X_processed.reshape((X_processed.shape[0], 1, X_processed.shape[1]))\n",
    "\n",
    "# Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp1, y_train, y_temp1 = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp1, y_temp1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Build the RNN model 1\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, activation='relu', return_sequences=True, \n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(SimpleRNN(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=2)\n",
    "\n",
    "# Save the model\n",
    "model.save('simple_rnn.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predicting\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print classification report or confusion matrix\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fb957",
   "metadata": {},
   "source": [
    "## Change the structure of RNN (Include LSTM and GRU) - Hybrid RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e15ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "4676/4676 [==============================] - 82s 16ms/step - loss: 0.6256 - accuracy: 0.6412 - val_loss: 0.5760 - val_accuracy: 0.6855 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5882 - accuracy: 0.6824 - val_loss: 0.5552 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5719 - accuracy: 0.6985 - val_loss: 0.5372 - val_accuracy: 0.7241 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5605 - accuracy: 0.7071 - val_loss: 0.5291 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5530 - accuracy: 0.7149 - val_loss: 0.5208 - val_accuracy: 0.7332 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5468 - accuracy: 0.7173 - val_loss: 0.5186 - val_accuracy: 0.7362 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5425 - accuracy: 0.7218 - val_loss: 0.5193 - val_accuracy: 0.7368 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5392 - accuracy: 0.7237 - val_loss: 0.5118 - val_accuracy: 0.7463 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5351 - accuracy: 0.7286 - val_loss: 0.5130 - val_accuracy: 0.7426 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5328 - accuracy: 0.7298 - val_loss: 0.5085 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5305 - accuracy: 0.7313 - val_loss: 0.5067 - val_accuracy: 0.7491 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5284 - accuracy: 0.7323 - val_loss: 0.5046 - val_accuracy: 0.7491 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5275 - accuracy: 0.7336 - val_loss: 0.5044 - val_accuracy: 0.7472 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "4676/4676 [==============================] - 77s 16ms/step - loss: 0.5251 - accuracy: 0.7356 - val_loss: 0.5035 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5235 - accuracy: 0.7366 - val_loss: 0.5041 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5202 - accuracy: 0.7374 - val_loss: 0.5007 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5197 - accuracy: 0.7390 - val_loss: 0.5007 - val_accuracy: 0.7525 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5186 - accuracy: 0.7396 - val_loss: 0.5001 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5163 - accuracy: 0.7407 - val_loss: 0.4979 - val_accuracy: 0.7547 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5166 - accuracy: 0.7409 - val_loss: 0.5003 - val_accuracy: 0.7519 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5134 - accuracy: 0.7428 - val_loss: 0.4957 - val_accuracy: 0.7543 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5118 - accuracy: 0.7439 - val_loss: 0.4965 - val_accuracy: 0.7528 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5114 - accuracy: 0.7441 - val_loss: 0.5001 - val_accuracy: 0.7532 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5106 - accuracy: 0.7439 - val_loss: 0.4954 - val_accuracy: 0.7526 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "4676/4676 [==============================] - 77s 16ms/step - loss: 0.5114 - accuracy: 0.7442 - val_loss: 0.4983 - val_accuracy: 0.7527 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5101 - accuracy: 0.7457 - val_loss: 0.4973 - val_accuracy: 0.7549 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "4676/4676 [==============================] - 77s 16ms/step - loss: 0.5084 - accuracy: 0.7470 - val_loss: 0.4921 - val_accuracy: 0.7578 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5071 - accuracy: 0.7471 - val_loss: 0.4923 - val_accuracy: 0.7552 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "4676/4676 [==============================] - 75s 16ms/step - loss: 0.5069 - accuracy: 0.7480 - val_loss: 0.4946 - val_accuracy: 0.7574 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "4676/4676 [==============================] - 74s 16ms/step - loss: 0.5058 - accuracy: 0.7489 - val_loss: 0.4902 - val_accuracy: 0.7579 - lr: 0.0010\n",
      "1002/1002 - 3s - loss: 0.4898 - accuracy: 0.7596 - 3s/epoch - 3ms/step\n",
      "Test Accuracy: 0.7596\n",
      "1002/1002 [==============================] - 3s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.75      0.76     15934\n",
      "        True       0.76      0.77      0.76     16129\n",
      "\n",
      "    accuracy                           0.76     32063\n",
      "   macro avg       0.76      0.76      0.76     32063\n",
      "weighted avg       0.76      0.76      0.76     32063\n",
      "\n",
      "[[11927  4007]\n",
      " [ 3700 12429]]\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model 2\n",
    "model = Sequential()\n",
    "\n",
    "# First RNN layer (LSTM)\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second RNN layer (GRU)\n",
    "model.add(GRU(100, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third RNN layer (LSTM)\n",
    "model.add(LSTM(50, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Save the model\n",
    "model.save('hybrid.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predicting\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print classification report or confusion matrix\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e92035",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "414d9cd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.8505 - accuracy: 0.6389\n",
      "Epoch 1: val_loss improved from inf to 0.62716, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 149s 29ms/step - loss: 0.8505 - accuracy: 0.6389 - val_loss: 0.6272 - val_accuracy: 0.6706 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6394 - accuracy: 0.6578\n",
      "Epoch 2: val_loss improved from 0.62716 to 0.62070, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 136s 29ms/step - loss: 0.6394 - accuracy: 0.6578 - val_loss: 0.6207 - val_accuracy: 0.6681 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.6586\n",
      "Epoch 3: val_loss improved from 0.62070 to 0.60969, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 137s 29ms/step - loss: 0.6337 - accuracy: 0.6586 - val_loss: 0.6097 - val_accuracy: 0.6773 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.6599\n",
      "Epoch 4: val_loss improved from 0.60969 to 0.60733, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 138s 29ms/step - loss: 0.6304 - accuracy: 0.6599 - val_loss: 0.6073 - val_accuracy: 0.6798 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.6302 - accuracy: 0.6592\n",
      "Epoch 5: val_loss did not improve from 0.60733\n",
      "4676/4676 [==============================] - 136s 29ms/step - loss: 0.6302 - accuracy: 0.6592 - val_loss: 0.6129 - val_accuracy: 0.6714 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.6593\n",
      "Epoch 6: val_loss did not improve from 0.60733\n",
      "4676/4676 [==============================] - 134s 29ms/step - loss: 0.6298 - accuracy: 0.6593 - val_loss: 0.6152 - val_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.6286 - accuracy: 0.6592\n",
      "Epoch 7: val_loss did not improve from 0.60733\n",
      "4676/4676 [==============================] - 135s 29ms/step - loss: 0.6286 - accuracy: 0.6592 - val_loss: 0.6077 - val_accuracy: 0.6769 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.6287 - accuracy: 0.6598\n",
      "Epoch 8: val_loss did not improve from 0.60733\n",
      "4676/4676 [==============================] - 135s 29ms/step - loss: 0.6287 - accuracy: 0.6598 - val_loss: 0.6090 - val_accuracy: 0.6871 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.6284 - accuracy: 0.6610\n",
      "Epoch 9: val_loss improved from 0.60733 to 0.60598, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 135s 29ms/step - loss: 0.6285 - accuracy: 0.6610 - val_loss: 0.6060 - val_accuracy: 0.6802 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.6610\n",
      "Epoch 10: val_loss improved from 0.60598 to 0.60144, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 138s 29ms/step - loss: 0.6276 - accuracy: 0.6610 - val_loss: 0.6014 - val_accuracy: 0.6840 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.6606\n",
      "Epoch 11: val_loss did not improve from 0.60144\n",
      "4676/4676 [==============================] - 135s 29ms/step - loss: 0.6268 - accuracy: 0.6606 - val_loss: 0.6029 - val_accuracy: 0.6831 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.6593\n",
      "Epoch 12: val_loss did not improve from 0.60144\n",
      "4676/4676 [==============================] - 135s 29ms/step - loss: 0.6273 - accuracy: 0.6593 - val_loss: 0.6052 - val_accuracy: 0.6780 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.6570\n",
      "Epoch 13: val_loss did not improve from 0.60144\n",
      "4676/4676 [==============================] - 138s 29ms/step - loss: 0.6283 - accuracy: 0.6570 - val_loss: 0.6085 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.6266 - accuracy: 0.6601\n",
      "Epoch 14: val_loss did not improve from 0.60144\n",
      "4676/4676 [==============================] - 105s 23ms/step - loss: 0.6266 - accuracy: 0.6601 - val_loss: 0.6047 - val_accuracy: 0.6842 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.6278 - accuracy: 0.6591\n",
      "Epoch 15: val_loss did not improve from 0.60144\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.6278 - accuracy: 0.6591 - val_loss: 0.6057 - val_accuracy: 0.6808 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.6737\n",
      "Epoch 16: val_loss improved from 0.60144 to 0.57833, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 98s 21ms/step - loss: 0.6081 - accuracy: 0.6738 - val_loss: 0.5783 - val_accuracy: 0.6994 - lr: 2.0000e-04\n",
      "Epoch 17/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.6013 - accuracy: 0.6765\n",
      "Epoch 17: val_loss improved from 0.57833 to 0.57135, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.6013 - accuracy: 0.6765 - val_loss: 0.5714 - val_accuracy: 0.7051 - lr: 2.0000e-04\n",
      "Epoch 18/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.5977 - accuracy: 0.6801\n",
      "Epoch 18: val_loss improved from 0.57135 to 0.56698, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5977 - accuracy: 0.6801 - val_loss: 0.5670 - val_accuracy: 0.7093 - lr: 2.0000e-04\n",
      "Epoch 19/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.6821\n",
      "Epoch 19: val_loss improved from 0.56698 to 0.56631, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 99s 21ms/step - loss: 0.5952 - accuracy: 0.6821 - val_loss: 0.5663 - val_accuracy: 0.7100 - lr: 2.0000e-04\n",
      "Epoch 20/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.6821\n",
      "Epoch 20: val_loss did not improve from 0.56631\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5942 - accuracy: 0.6821 - val_loss: 0.5670 - val_accuracy: 0.7110 - lr: 2.0000e-04\n",
      "Epoch 21/30\n",
      "4675/4676 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.6839\n",
      "Epoch 21: val_loss improved from 0.56631 to 0.56548, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5932 - accuracy: 0.6839 - val_loss: 0.5655 - val_accuracy: 0.7097 - lr: 2.0000e-04\n",
      "Epoch 22/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.6841\n",
      "Epoch 22: val_loss did not improve from 0.56548\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5922 - accuracy: 0.6841 - val_loss: 0.5656 - val_accuracy: 0.7078 - lr: 2.0000e-04\n",
      "Epoch 23/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.6831\n",
      "Epoch 23: val_loss improved from 0.56548 to 0.56495, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5919 - accuracy: 0.6831 - val_loss: 0.5649 - val_accuracy: 0.7109 - lr: 2.0000e-04\n",
      "Epoch 24/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.6842\n",
      "Epoch 24: val_loss improved from 0.56495 to 0.56392, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5920 - accuracy: 0.6842 - val_loss: 0.5639 - val_accuracy: 0.7107 - lr: 2.0000e-04\n",
      "Epoch 25/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.6841\n",
      "Epoch 25: val_loss improved from 0.56392 to 0.56044, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5924 - accuracy: 0.6841 - val_loss: 0.5604 - val_accuracy: 0.7157 - lr: 2.0000e-04\n",
      "Epoch 26/30\n",
      "4676/4676 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.6855\n",
      "Epoch 26: val_loss did not improve from 0.56044\n",
      "4676/4676 [==============================] - 98s 21ms/step - loss: 0.5917 - accuracy: 0.6855 - val_loss: 0.5648 - val_accuracy: 0.7098 - lr: 2.0000e-04\n",
      "Epoch 27/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5907 - accuracy: 0.6856\n",
      "Epoch 27: val_loss did not improve from 0.56044\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5907 - accuracy: 0.6856 - val_loss: 0.5637 - val_accuracy: 0.7099 - lr: 2.0000e-04\n",
      "Epoch 28/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.6843\n",
      "Epoch 28: val_loss did not improve from 0.56044\n",
      "4676/4676 [==============================] - 99s 21ms/step - loss: 0.5926 - accuracy: 0.6843 - val_loss: 0.5607 - val_accuracy: 0.7108 - lr: 2.0000e-04\n",
      "Epoch 29/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.6839\n",
      "Epoch 29: val_loss improved from 0.56044 to 0.56025, saving model to best_model_stacked.h5\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5921 - accuracy: 0.6839 - val_loss: 0.5602 - val_accuracy: 0.7128 - lr: 2.0000e-04\n",
      "Epoch 30/30\n",
      "4674/4676 [============================>.] - ETA: 0s - loss: 0.5912 - accuracy: 0.6851\n",
      "Epoch 30: val_loss did not improve from 0.56025\n",
      "4676/4676 [==============================] - 97s 21ms/step - loss: 0.5913 - accuracy: 0.6851 - val_loss: 0.5613 - val_accuracy: 0.7140 - lr: 2.0000e-04\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1002/1002 - 4s - loss: 0.5591 - accuracy: 0.7127 - 4s/epoch - 4ms/step\n",
      "Test Accuracy: 0.7127\n",
      "1002/1002 [==============================] - 4s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.66      0.70     15934\n",
      "        True       0.69      0.76      0.73     16129\n",
      "\n",
      "    accuracy                           0.71     32063\n",
      "   macro avg       0.71      0.71      0.71     32063\n",
      "weighted avg       0.71      0.71      0.71     32063\n",
      "\n",
      "[[10521  5413]\n",
      " [ 3799 12330]]\n"
     ]
    }
   ],
   "source": [
    "# Define the enhanced stacked LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True, \n",
    "                             kernel_regularizer=l2(0.01)), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True, \n",
    "                             kernel_regularizer=l2(0.01))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_stacked.h5', monitor='val_loss', \n",
    "                                   save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping, reduce_lr,model_checkpoint])\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model('best_model_stacked.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predicting\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print classification report or confusion matrix\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876154f",
   "metadata": {},
   "source": [
    "### SDA-LSTM (previously SDA-LM) Method\n",
    "\n",
    "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00380-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2e4064d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18703/18703 [==============================] - 63s 3ms/step - loss: 0.0020 - val_loss: 2.2096e-04\n",
      "Epoch 2/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 2.1477e-04 - val_loss: 2.0239e-04\n",
      "Epoch 3/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 1.9627e-04 - val_loss: 1.8808e-04\n",
      "Epoch 4/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 1.8353e-04 - val_loss: 1.8212e-04\n",
      "Epoch 5/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 1.7186e-04 - val_loss: 1.5436e-04\n",
      "Epoch 6/10\n",
      "18703/18703 [==============================] - 63s 3ms/step - loss: 1.4173e-04 - val_loss: 1.2803e-04\n",
      "Epoch 7/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 1.1710e-04 - val_loss: 1.1406e-04\n",
      "Epoch 8/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 1.0727e-04 - val_loss: 1.0566e-04\n",
      "Epoch 9/10\n",
      "18703/18703 [==============================] - 60s 3ms/step - loss: 1.0435e-04 - val_loss: 1.0250e-04\n",
      "Epoch 10/10\n",
      "18703/18703 [==============================] - 60s 3ms/step - loss: 1.0054e-04 - val_loss: 9.7606e-05\n",
      "46756/46756 [==============================] - 33s 698us/step\n",
      "10020/10020 [==============================] - 7s 701us/step\n",
      "10020/10020 [==============================] - 7s 704us/step\n",
      "Epoch 1/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7928 - val_loss: 0.7874\n",
      "Epoch 2/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7857 - val_loss: 0.7847\n",
      "Epoch 3/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 0.7843 - val_loss: 0.7838\n",
      "Epoch 4/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 0.7828 - val_loss: 0.7825\n",
      "Epoch 5/10\n",
      "18703/18703 [==============================] - 58s 3ms/step - loss: 0.7824 - val_loss: 0.7825\n",
      "Epoch 6/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7823 - val_loss: 0.7824\n",
      "Epoch 7/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7822 - val_loss: 0.7821\n",
      "Epoch 8/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7820 - val_loss: 0.7820\n",
      "Epoch 9/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.7818 - val_loss: 0.7819\n",
      "Epoch 10/10\n",
      "18703/18703 [==============================] - 58s 3ms/step - loss: 0.7818 - val_loss: 0.7818\n",
      "46756/46756 [==============================] - 33s 697us/step\n",
      "10020/10020 [==============================] - 7s 709us/step\n",
      "10020/10020 [==============================] - 7s 702us/step\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/2\n",
      "2338/2338 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9903\n",
      "Epoch 1: val_loss improved from inf to 0.00973, saving model to best_model_sda_lm.h5\n",
      "2338/2338 [==============================] - 162s 67ms/step - loss: 0.0295 - accuracy: 0.9903 - val_loss: 0.0097 - val_accuracy: 0.9965 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "2338/2338 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 2: val_loss did not improve from 0.00973\n",
      "2338/2338 [==============================] - 157s 67ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0169 - val_accuracy: 0.9948 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed2 = preprocessor2.fit_transform(X)\n",
    "\n",
    "# Add the target column back\n",
    "data = np.hstack((X_processed2, y.values.reshape(-1, 1)))\n",
    "\n",
    "# Convert data to sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-1])\n",
    "        y.append(data[i+seq_length, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 10  # Example sequence length\n",
    "X2, y2 = create_sequences(data, seq_length)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train2, X_temp, y_train2, y_temp = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "X_val2, X_test2, y_val2, y_test2 = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define denoising autoencoder with separate encoder and decoder\n",
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    # Decoder\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded_input)\n",
    "    decoder = Model(encoded_input, decoded)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_layer, decoder(encoder(input_layer)))\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "input_dim = X_train2.shape[2]  # Number of features\n",
    "encoding_dim_1 = 64  # First encoding dimension\n",
    "encoding_dim_2 = 32  # Second encoding dimension\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Build first autoencoder\n",
    "autoencoder1, encoder1, decoder1 = build_autoencoder(input_dim, encoding_dim_1)\n",
    "autoencoder1.compile(optimizer='adam', loss='mse')\n",
    "autoencoder1.fit(X_train2.reshape(-1, input_dim), \n",
    "                 X_train2.reshape(-1, input_dim), \n",
    "                 epochs=10, \n",
    "                 batch_size=64, \n",
    "                 shuffle=True, \n",
    "                 validation_split=0.2, \n",
    "                 callbacks=[early_stopping])\n",
    "\n",
    "# Encode the data\n",
    "encoded_train1 = encoder1.predict(X_train2.reshape(-1, input_dim))\n",
    "encoded_val1 = encoder1.predict(X_val2.reshape(-1, input_dim))\n",
    "encoded_test1 = encoder1.predict(X_test2.reshape(-1, input_dim))\n",
    "\n",
    "# Build second autoencoder\n",
    "autoencoder2, encoder2, decoder2 = build_autoencoder(encoding_dim_1, encoding_dim_2)\n",
    "autoencoder2.compile(optimizer='adam', loss='mse')\n",
    "autoencoder2.fit(encoded_train1, \n",
    "                 encoded_train1, \n",
    "                 epochs=10, \n",
    "                 batch_size=64, \n",
    "                 shuffle=True, \n",
    "                 validation_split=0.2, \n",
    "                 callbacks=[early_stopping])\n",
    "\n",
    "# Encode the data again\n",
    "encoded_train2 = encoder2.predict(encoded_train1).reshape(X_train2.shape[0], seq_length, encoding_dim_2)\n",
    "encoded_val2 = encoder2.predict(encoded_val1).reshape(X_val2.shape[0], seq_length, encoding_dim_2)\n",
    "encoded_test2 = encoder2.predict(encoded_test1).reshape(X_test2.shape[0], seq_length, encoding_dim_2)\n",
    "\n",
    "# Build and compile the final model with LSTM layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, encoding_dim_2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_sda_lm.h5', monitor='val_loss', \n",
    "                                   save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(encoded_train2, y_train2, \n",
    "                    epochs=2, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(encoded_val2, y_val2), \n",
    "                    callbacks=[early_stopping, reduce_lr, model_checkpoint])\n",
    "\n",
    "# Save the autoencoders\n",
    "autoencoder1.save('autoencoder1.h5')\n",
    "autoencoder2.save('autoencoder2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = load_model('best_model_sda_lm.h5')\n",
    "\n",
    "# Evaluate the best model\n",
    "loss, accuracy = best_model.evaluate(encoded_test2, y_test2)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Classification report for the best model\n",
    "y_pred = (best_model.predict(encoded_test2) > 0.5).astype(\"int32\")\n",
    "print(confusion_matrix(y_test2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbe1c5",
   "metadata": {},
   "source": [
    "## Apply the same methodology in the Imbalanced Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cfbd525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 0.0019 - val_loss: 2.2277e-04\n",
      "Epoch 2/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 2.1176e-04 - val_loss: 2.0681e-04\n",
      "Epoch 3/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 2.0659e-04 - val_loss: 2.0255e-04\n",
      "Epoch 4/10\n",
      "18703/18703 [==============================] - 60s 3ms/step - loss: 1.7760e-04 - val_loss: 1.4747e-04\n",
      "Epoch 5/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 1.4579e-04 - val_loss: 1.4546e-04\n",
      "Epoch 6/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 1.4301e-04 - val_loss: 1.2332e-04\n",
      "Epoch 7/10\n",
      "18703/18703 [==============================] - 57s 3ms/step - loss: 1.1487e-04 - val_loss: 1.0905e-04\n",
      "Epoch 8/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 1.0616e-04 - val_loss: 1.0687e-04\n",
      "Epoch 9/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 1.0391e-04 - val_loss: 1.0211e-04\n",
      "Epoch 10/10\n",
      "18703/18703 [==============================] - 60s 3ms/step - loss: 1.0221e-04 - val_loss: 1.0094e-04\n",
      "46756/46756 [==============================] - 32s 691us/step\n",
      "10020/10020 [==============================] - 7s 695us/step\n",
      "10020/10020 [==============================] - 7s 698us/step\n",
      "Epoch 1/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 0.6311 - val_loss: 0.6244\n",
      "Epoch 2/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 0.6240 - val_loss: 0.6237\n",
      "Epoch 3/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 0.6237 - val_loss: 0.6236\n",
      "Epoch 4/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 0.6236 - val_loss: 0.6236\n",
      "Epoch 5/10\n",
      "18703/18703 [==============================] - 61s 3ms/step - loss: 0.6236 - val_loss: 0.6235\n",
      "Epoch 6/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 0.6236 - val_loss: 0.6235\n",
      "Epoch 7/10\n",
      "18703/18703 [==============================] - 63s 3ms/step - loss: 0.6235 - val_loss: 0.6235\n",
      "Epoch 8/10\n",
      "18703/18703 [==============================] - 59s 3ms/step - loss: 0.6235 - val_loss: 0.6235\n",
      "Epoch 9/10\n",
      "18703/18703 [==============================] - 60s 3ms/step - loss: 0.6235 - val_loss: 0.6235\n",
      "Epoch 10/10\n",
      "18703/18703 [==============================] - 62s 3ms/step - loss: 0.6235 - val_loss: 0.6235\n",
      "46756/46756 [==============================] - 33s 694us/step\n",
      "10020/10020 [==============================] - 7s 702us/step\n",
      "10020/10020 [==============================] - 7s 700us/step\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/2\n",
      "2338/2338 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9863\n",
      "Epoch 1: val_loss improved from inf to 0.02501, saving model to best_model_sda_lm.h5\n",
      "2338/2338 [==============================] - 163s 68ms/step - loss: 0.0376 - accuracy: 0.9863 - val_loss: 0.0250 - val_accuracy: 0.9920 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "2338/2338 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 2: val_loss improved from 0.02501 to 0.00447, saving model to best_model_sda_lm.h5\n",
      "2338/2338 [==============================] - 158s 67ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0045 - val_accuracy: 0.9989 - lr: 0.0010\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 10, 32), found shape=(None, 10, 55)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m best_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_sda_lm.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Evaluate the best model\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Classification report for the best model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekmiy69ar.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/nfs/home/mnk1906/.local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 10, 32), found shape=(None, 10, 55)\n"
     ]
    }
   ],
   "source": [
    "X2 = clean_data.drop(columns='Cancelled',axis=1)\n",
    "y2 = clean_data['Cancelled']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X2.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X2.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor2_imb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed2_imb = preprocessor2_imb.fit_transform(X2)\n",
    "\n",
    "# Add the target column back\n",
    "data2 = np.hstack((X_processed2, y2.values.reshape(-1, 1)))\n",
    "\n",
    "seq_length = 10  # Example sequence length\n",
    "X2, y2 = create_sequences(data2, seq_length)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train2, X_temp, y_train2, y_temp = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "X_val2, X_test2, y_val2, y_test2 = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "input_dim = X_train2.shape[2]  # Number of features\n",
    "encoding_dim_1 = 64  # First encoding dimension\n",
    "encoding_dim_2 = 32  # Second encoding dimension\n",
    "\n",
    "# Build first autoencoder\n",
    "autoencoder1, encoder1, decoder1 = build_autoencoder(input_dim, encoding_dim_1)\n",
    "autoencoder1.compile(optimizer='adam', loss='mse')\n",
    "autoencoder1.fit(X_train2.reshape(-1, input_dim), \n",
    "                 X_train2.reshape(-1, input_dim), \n",
    "                 epochs=10, \n",
    "                 batch_size=64, \n",
    "                 shuffle=True, \n",
    "                 validation_split=0.2, \n",
    "                 callbacks=[early_stopping])\n",
    "\n",
    "# Encode the data\n",
    "encoded_train1 = encoder1.predict(X_train2.reshape(-1, input_dim))\n",
    "encoded_val1 = encoder1.predict(X_val2.reshape(-1, input_dim))\n",
    "encoded_test1 = encoder1.predict(X_test2.reshape(-1, input_dim))\n",
    "\n",
    "# Build second autoencoder\n",
    "autoencoder2, encoder2, decoder2 = build_autoencoder(encoding_dim_1, encoding_dim_2)\n",
    "autoencoder2.compile(optimizer='adam', loss='mse')\n",
    "autoencoder2.fit(encoded_train1, \n",
    "                 encoded_train1, \n",
    "                 epochs=10, \n",
    "                 batch_size=64, \n",
    "                 shuffle=True, \n",
    "                 validation_split=0.2, \n",
    "                 callbacks=[early_stopping])\n",
    "\n",
    "# Encode the data again\n",
    "encoded_train2 = encoder2.predict(encoded_train1).reshape(X_train2.shape[0], seq_length, encoding_dim_2)\n",
    "encoded_val2 = encoder2.predict(encoded_val1).reshape(X_val2.shape[0], seq_length, encoding_dim_2)\n",
    "encoded_test2 = encoder2.predict(encoded_test1).reshape(X_test2.shape[0], seq_length, encoding_dim_2)\n",
    "\n",
    "# Build and compile the final model with LSTM layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, encoding_dim_2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_sda_lm.h5', monitor='val_loss', \n",
    "                                   save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(encoded_train2, y_train2, \n",
    "                    epochs=2, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(encoded_val2, y_val2), \n",
    "                    callbacks=[early_stopping, reduce_lr, model_checkpoint])\n",
    "\n",
    "# Save the autoencoders\n",
    "autoencoder1.save('autoencoder1.h5')\n",
    "autoencoder2.save('autoencoder2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c8ca5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1002/1002 [==============================] - 13s 12ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Test Loss: 0.004196505062282085, Test Accuracy: 0.9988459944725037\n",
      "1002/1002 [==============================] - 11s 11ms/step\n",
      "[[15877     1]\n",
      " [   36 16148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     15878\n",
      "         1.0       1.00      1.00      1.00     16184\n",
      "\n",
      "    accuracy                           1.00     32062\n",
      "   macro avg       1.00      1.00      1.00     32062\n",
      "weighted avg       1.00      1.00      1.00     32062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = load_model('best_model_sda_lm.h5')\n",
    "\n",
    "# Evaluate the best model\n",
    "loss, accuracy = best_model.evaluate(encoded_test2, y_test2)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Classification report for the best model\n",
    "y_pred = (best_model.predict(encoded_test2) > 0.5).astype(\"int32\")\n",
    "print(confusion_matrix(y_test2, y_pred))\n",
    "print(classification_report(y_test2, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
